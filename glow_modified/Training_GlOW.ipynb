{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7cdd5e-9296-4b5f-a20c-7b229336532f",
   "metadata": {},
   "source": [
    "1. https://github.com/rosinality/glow-pytorch\n",
    "2. https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html\n",
    "3. https://blog.evjang.com/2018/01/nf1.html\n",
    "4. https://blog.evjang.com/2018/01/nf2.html\n",
    "5. http://akosiorek.github.io/ml/2018/04/03/norm_flows.html\n",
    "6. https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial11/NF_image_modeling.html\n",
    "7. https://www.youtube.com/playlist?list=PLdlPlO1QhMiAkedeu0aJixfkknLRxk1nA\n",
    "8. https://ml4a.github.io/classes/itp-F18/09/#\n",
    "9. https://openai.com/blog/glow/\n",
    "10. https://www.youtube.com/watch?v=kIvyex9HH0Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46243aac-b4c5-4d41-806a-5e4f462f4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "# import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import graphics\n",
    "from utils import ResultLogger\n",
    "import matplotlib.pyplot as plt\n",
    "from recordtype import recordtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85b28e-3e00-46df-8d0f-d6284d5e913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hps1 = recordtype('hps1', ['problem','seed','logdir','dal','image_size','lr','n_levels','direct_iterator','epochs',\n",
    "                           'epochs_warmup','epochs_full_valid','epochs_full_sample','n_batch_train','local_batch_train',\n",
    "                           'local_batch_test','n_test','n_y','n_train','train_its','test_its','full_test_its','inference',\n",
    "                           'rnd_crop','anchor_size','n_batch_init','local_batch_init','n_bits_x','n_bins',\n",
    "                           'flow_permutation','flow_coupling','depth','width','top_shape','learntop','ycond',\n",
    "                           'weight_y','gradient_checkpointing','optimizer','polyak_epochs','beta1',\n",
    "                           'weight_decay','restore_path','category', 'data_dir','n_batch_test','n_sample',\n",
    "                           'verbose'])\n",
    "\n",
    "# Our new \"Car\" class works as expected:\n",
    "hps = hps1('mnist',1,'./logs',1,32,0.001,5,False,200,\n",
    "           10,50,50,64,50,\n",
    "           None,100,10,1500,None,None,None,False,\n",
    "           None,32,256,None,5,None,\n",
    "           2,0,8,512,None,True,False,\n",
    "           0.0,1,'adamax',1,0.9,\n",
    "           1.0,'','','',50,1,\n",
    "           True)\n",
    "\n",
    "\n",
    "hvd1 = recordtype('hps1', ['size','rank'])\n",
    "hvd = hvd1(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9842871-6e8e-45a5-b47a-59b841d35e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def align_images(x,hps):\n",
    "#     x = imutils.resize(x, width=hps.image_size, height=hps.image_size)\n",
    "#     x = x.reshape(hps.image_size, hps.image_size, 1)\n",
    "#     return x\n",
    "\n",
    "# x_train = np.array([align_images(x,hps) for x in x_train])\n",
    "# x_test = np.array([align_images(x,hps) for x in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2bf9f-dfed-4f57-a982-dda98ddf2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_session():\n",
    "    # Init session and params\n",
    "    sess = tf.Session()\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1d105-ebbe-43fc-bb3e-afb923f5f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(hps, sess):\n",
    "    if hps.problem == 'lsun_realnvp':\n",
    "        hps.rnd_crop = True\n",
    "    else:\n",
    "        hps.rnd_crop = False\n",
    "\n",
    "    # Use anchor_size to rescale batch size based on image_size\n",
    "    s = hps.anchor_size\n",
    "    hps.local_batch_train = hps.n_batch_train * \\\n",
    "        s * s // (hps.image_size * hps.image_size)\n",
    "\n",
    "    hps.local_batch_test = {64: 50, 32: 25, 16: 10, 8: 5, 4: 2, 2: 2, 1: 1}[\n",
    "        hps.local_batch_train]  # round down to closest divisor of 50\n",
    "      \n",
    "    hps.local_batch_init = hps.n_batch_init * \\\n",
    "        s * s // (hps.image_size * hps.image_size)\n",
    "\n",
    "    print(\"Rank {} Batch sizes Train {} Test {} Init {}\".format(\n",
    "        hvd.rank, hps.local_batch_train, hps.local_batch_test, hps.local_batch_init))\n",
    "\n",
    "    if hps.problem in ['imagenet-oord', 'imagenet', 'celeba', 'lsun_realnvp', 'lsun']:\n",
    "        hps.direct_iterator = True\n",
    "        import data_loaders.get_data as v\n",
    "        train_iterator, test_iterator, data_init = \\\n",
    "            v.get_data(sess, hps.data_dir, hvd.size, hvd.rank, hps.pmap, hps.fmap, hps.local_batch_train,\n",
    "                       hps.local_batch_test, hps.local_batch_init, hps.image_size, hps.rnd_crop)\n",
    "\n",
    "    elif hps.problem in ['mnist', 'cifar10']:\n",
    "        hps.direct_iterator = False\n",
    "        import data_loaders.get_mnist_cifar as v\n",
    "        train_iterator, test_iterator, data_init = \\\n",
    "            v.get_data(hps.problem, hvd.size, hvd.rank, hps.dal, hps.local_batch_train,\n",
    "                       hps.local_batch_test, hps.local_batch_init, hps.image_size)\n",
    "\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "    return train_iterator, test_iterator, data_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ba2b6-53d5-409c-88ad-4f487b5dd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_its(hps):\n",
    "    # These run for a fixed amount of time. As anchored batch is smaller, we've actually seen fewer examples\n",
    "    train_its = int(np.ceil(hps.n_train / (hps.n_batch_train * hvd.size)))\n",
    "    test_its = int(np.ceil(hps.n_test / (hps.n_batch_train * hvd.size)))\n",
    "    train_epoch = train_its * hps.n_batch_train * hvd.size\n",
    "\n",
    "    # Do a full validation run\n",
    "    if hvd.rank == 0:\n",
    "        print(hps.n_test, hps.local_batch_test, hvd.size)\n",
    "    assert hps.n_test % (hps.local_batch_test * hvd.size) == 0\n",
    "    full_test_its = hps.n_test // (hps.local_batch_test * hvd.size)\n",
    "\n",
    "    if hvd.rank == 0:\n",
    "        print(\"Train epoch size: \" + str(train_epoch))\n",
    "    return train_its, test_its, full_test_its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097a0ee-8832-4105-82f2-e46953717383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(sess, model, hps, iterator):\n",
    "    # Example of using model in inference mode. Load saved model using hps.restore_path\n",
    "    # Can provide x, y from files instead of dataset iterator\n",
    "    # If model is uncondtional, always pass y = np.zeros([bs], dtype=np.int32)\n",
    "    if hps.direct_iterator:\n",
    "        iterator = iterator.get_next()\n",
    "\n",
    "    xs = []\n",
    "    zs = []\n",
    "    for it in range(hps.full_test_its):\n",
    "        if hps.direct_iterator:\n",
    "            # replace with x, y, attr if you're getting CelebA attributes, also modify get_data\n",
    "            x, y = sess.run(iterator)\n",
    "        else:\n",
    "            x, y = iterator()\n",
    "\n",
    "        z = model.encode(x, y)\n",
    "        x = model.decode(y, z)\n",
    "        xs.append(x)\n",
    "        zs.append(z)\n",
    "\n",
    "    x = np.concatenate(xs, axis=0)\n",
    "    z = np.concatenate(zs, axis=0)\n",
    "    np.save('logs/x.npy', x)\n",
    "    np.save('logs/z.npy', z)\n",
    "    return zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701481d4-9456-4b3c-bcbd-e8870fb26a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, model, hps, logdir, visualise):\n",
    "    _print(hps)\n",
    "    _print('Starting training. Logging to', logdir)\n",
    "    _print('epoch n_processed n_images ips dtrain dtest dsample dtot train_results test_results msg')\n",
    "\n",
    "    # Train\n",
    "    sess.graph.finalize()\n",
    "    n_processed = 0\n",
    "    n_images = 0\n",
    "    train_time = 0.0\n",
    "    test_loss_best = 999999\n",
    "   \n",
    "    train_logger = ResultLogger(logdir + \"train.txt\", hps)\n",
    "    test_logger = ResultLogger(logdir + \"test.txt\", hps)\n",
    "\n",
    "    \n",
    "    tcurr = time.time()\n",
    "    for epoch in range(1, hps.epochs):\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        train_results = []\n",
    "        for it in range(hps.train_its):\n",
    "\n",
    "            # Set learning rate, linearly annealed from 0 in the first hps.epochs_warmup epochs.\n",
    "            lr = hps.lr * min(1., n_processed /\n",
    "                              (hps.n_train * hps.epochs_warmup))\n",
    "\n",
    "            # Run a training step synchronously.\n",
    "            _t = time.time()\n",
    "            train_results += [model.train(lr)]\n",
    "            \n",
    "            _print(n_processed, time.time()-_t, train_results[-1])\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Images seen wrt anchor resolution\n",
    "            n_processed += hps.n_batch_train\n",
    "            # Actual images seen at current resolution\n",
    "            n_images += hps.local_batch_train\n",
    "\n",
    "        train_results = np.mean(np.asarray(train_results), axis=0)\n",
    "\n",
    "        dtrain = time.time() - t\n",
    "        ips = (hps.train_its * hps.local_batch_train) / dtrain\n",
    "        train_time += dtrain\n",
    "\n",
    "        if hvd.rank == 0:\n",
    "            train_logger.log(epoch=epoch, n_processed=n_processed, n_images=n_images, train_time=int(\n",
    "                train_time), **process_results(train_results))\n",
    "\n",
    "        if epoch < 10 or (epoch < 50 and epoch % 10 == 0) or epoch % hps.epochs_full_valid == 0:\n",
    "            test_results = []\n",
    "            msg = ''\n",
    "\n",
    "            t = time.time()\n",
    "            # model.polyak_swap()\n",
    "\n",
    "            if epoch % hps.epochs_full_valid == 0:\n",
    "                # Full validation run\n",
    "                for it in range(hps.full_test_its):\n",
    "                    test_results += [model.test()]\n",
    "                test_results = np.mean(np.asarray(test_results), axis=0)\n",
    "\n",
    "                if hvd.rank == 0:\n",
    "                    test_logger.log(epoch=epoch, n_processed=n_processed,\n",
    "                                    n_images=n_images, **process_results(test_results))\n",
    "\n",
    "                    # Save checkpoint\n",
    "                    if test_results[0] < test_loss_best:\n",
    "                        test_loss_best = test_results[0]\n",
    "                        model.save(logdir+\"model_best_loss.ckpt\")\n",
    "                        msg += ' *'\n",
    "\n",
    "            dtest = time.time() - t\n",
    "\n",
    "            # Sample\n",
    "            t = time.time()\n",
    "            if epoch == 1 or epoch == 10 or epoch % hps.epochs_full_sample == 0:\n",
    "                visualise(epoch)\n",
    "            dsample = time.time() - t\n",
    "\n",
    "            dcurr = time.time() - tcurr\n",
    "            tcurr = time.time()\n",
    "            _print(epoch, n_processed, n_images, \"{:.1f} {:.1f} {:.1f} {:.1f} {:.1f}\".format(\n",
    "                ips, dtrain, dtest, dsample, dcurr), train_results, test_results, msg)\n",
    "\n",
    "            # model.polyak_swap()\n",
    "\n",
    "    \n",
    "    _print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d193d-811b-40c6-b1c7-3c40ddf42140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results):\n",
    "    stats = ['loss', 'bits_x', 'bits_y', 'pred_loss']\n",
    "    assert len(stats) == results.shape[0]\n",
    "    res_dict = {}\n",
    "    for i in range(len(stats)):\n",
    "        res_dict[stats[i]] = \"{:.4f}\".format(results[i])\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38de15a-c694-4b62-a74c-5373ae77808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print(*args, **kwargs):\n",
    "    print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dc035-d344-486b-b864-905954289941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_visualizations(hps, model, logdir):\n",
    "\n",
    "    def sample_batch(y, eps):\n",
    "        n_batch = hps.local_batch_train\n",
    "        xs = []\n",
    "        for i in range(int(np.ceil(len(eps) / n_batch))):\n",
    "            xs.append(model.sample(\n",
    "                y[i*n_batch:i*n_batch + n_batch], eps[i*n_batch:i*n_batch + n_batch]))\n",
    "        return np.concatenate(xs)\n",
    "\n",
    "    def draw_samples(epoch):\n",
    "        if hvd.rank != 0:\n",
    "            return\n",
    "\n",
    "        rows = 10 if hps.image_size <= 64 else 4\n",
    "        cols = rows\n",
    "        n_batch = rows*cols\n",
    "        y = np.asarray([_y % hps.n_y for _y in (\n",
    "            list(range(cols)) * rows)], dtype='int32')\n",
    "\n",
    "        # temperatures = [0., .25, .5, .626, .75, .875, 1.] #previously\n",
    "        temperatures = [0., .25, .5, .6, .7, .8, .9, 1.]\n",
    "\n",
    "        x_samples = []\n",
    "        x_samples.append(sample_batch(y, [.0]*n_batch))\n",
    "        x_samples.append(sample_batch(y, [.25]*n_batch))\n",
    "        x_samples.append(sample_batch(y, [.5]*n_batch))\n",
    "        x_samples.append(sample_batch(y, [.6]*n_batch))\n",
    "        x_samples.append(sample_batch(y, [.7]*n_batch))\n",
    "        x_samples.append(sample_batch(y, [.8]*n_batch))\n",
    "        x_samples.append(sample_batch(y, [.9]*n_batch))\n",
    "        x_samples.append(sample_batch(y, [1.]*n_batch))\n",
    "        # previously: 0, .25, .5, .625, .75, .875, 1.\n",
    "\n",
    "        for i in range(len(x_samples)):\n",
    "            x_sample = np.reshape(\n",
    "                x_samples[i], (n_batch, hps.image_size, hps.image_size, 3))\n",
    "            graphics.save_raster(x_sample, logdir +\n",
    "                                 'epoch_{}_sample_{}.png'.format(epoch, i))\n",
    "\n",
    "    return draw_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e588f-1b9d-4737-9a7e-8658f9068e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn = tf.contrib.learn\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "# Surpress verbose warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "sess = tensorflow_session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "seed = hps.seed\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "image_size = hps.image_size\n",
    "n_train = hps.n_train\n",
    "n_test = hps.n_test\n",
    "n_y = hps.n_y\n",
    "\n",
    "problem = hps.problem\n",
    "dal = hps.dal\n",
    "n_levels = hps.n_levels\n",
    "depth = hps.depth\n",
    "lr = hps.lr\n",
    "logdir = hps.logdir\n",
    "\n",
    "train_iterator, test_iterator, data_init = get_data(hps, sess)\n",
    "\n",
    "hps.train_its, hps.test_its, hps.full_test_its = get_its(hps)\n",
    "\n",
    "# Create log dir\n",
    "logdir = os.path.abspath(hps.logdir) + \"/\"\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "\n",
    "import model\n",
    "model = model.model(sess, hps, train_iterator, test_iterator, data_init)\n",
    "\n",
    "# Initialize visualization functions\n",
    "visualise = init_visualizations(hps, model, logdir)\n",
    "\n",
    "if not hps.inference:\n",
    "    # Perform training\n",
    "    train(sess, model, hps, logdir, visualise)\n",
    "else:\n",
    "    infer(sess, model, hps, test_iterator)\n",
    "#     logp_ = model.logp\n",
    "    print(logp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2af6a9-b426-423e-b7f0-eda7e3e0513e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842280d-384b-473d-a5f7-b2a5a0a2bb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968b2c0-3057-463e-ad17-3be1d9691e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e091180-8043-46ec-98df-cad2b11af428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
